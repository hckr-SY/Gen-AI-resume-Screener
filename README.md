# ðŸ§  GenAI Resume Screener

A Streamlit-based web application that uses Generative AI and semantic similarity to intelligently match candidate resumes with job descriptions.

---

## ðŸš€ Features

- LLM-based parsing of job descriptions and resumes (using Ollama + Mistral)
- Semantic cosine similarity scoring using Sentence Transformers
- Multi-agent architecture (JD parser, CV extractor, matcher, UI)
- SQLite-powered resume database
- Streamlit UI for viewing top-matching candidates

---

## ðŸ›  Technologies Used

- Python
- Streamlit
- Ollama (Mistral model)
- SentenceTransformers
- scikit-learn
- PyMuPDF
- SQLite
- Pandas

---

## ðŸ§± Project Structure

project_hackathon/
â”œâ”€â”€ app.py                     # Streamlit web app
â”œâ”€â”€ precompute_jds.py          # Parses JDs via Ollama
â”œâ”€â”€ precompute_cvs.py          # âœ… Parses resumes and adds to DB
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ matcher.py             # Resume-JD matcher
â”‚   â””â”€â”€ cv_extractor.py        # Resume extractor using LLM
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ jd_loader.py           # Load parsed JD cache
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ allCV/                 # Folder with PDF resumes
â”‚   â”œâ”€â”€ job_description.csv    # Raw JDs
â”‚   â”œâ”€â”€ jd_cache.pkl           # Pre-parsed JD cache
â”‚   â””â”€â”€ jobmatch.db            # Parsed resume DB

ðŸš€ How to Run the Project
Clone the repository

bash
Copy
Edit
git clone https://github.com/hckr-SY/Gen-AI-resume-Screener.git
cd Gen-AI-resume-Screener
Install dependencies

Make sure you have Python 3.9+ installed.

nginx
Copy
Edit
pip install -r requirements.txt
Start Ollama (LLM must be available locally)

Pull and start the required model (e.g. mistral):

nginx
Copy
Edit
ollama pull mistral
Make sure ollama is running in the background.

(Optional) Preprocess Job Descriptions

If you want to cache parsed JDs:

nginx
Copy
Edit
python precompute_jds.py
(Optional) Preprocess Resumes

To parse all resumes from data/allCV/ and store them in the database:

nginx
Copy
Edit
python precompute_cvs.py
Run the Web App

arduino
Copy
Edit
streamlit run app.py